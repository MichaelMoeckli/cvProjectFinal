{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f506f6",
   "metadata": {},
   "source": [
    "# üîç Compare Trained Model vs Zero-Shot CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61ab13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3861365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbd3564e58d4b7fbf0189b683d3e682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b356bdc8174d41bfd660faadcf915f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094b18d7dd0b4359827badd4696f5c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4410b118d3c44155a1a5cbae42e76f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f657a88568c4c8184d96ced58d75896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2e15a77be6415692adefe18901a38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d8fd70d99c48188ec89140a855985a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb8f11e44c944ab873c711b312e6d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b404d39c0e004e4c81d534e7aa64cad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_model = clip_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa2022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"a photo of a butterfly\",\n",
    "    \"a photo of a cat\",\n",
    "    \"a photo of a chicken\",\n",
    "    \"a photo of a cow\",\n",
    "    \"a photo of a dog\",\n",
    "    \"a photo of an elephant\",\n",
    "    \"a photo of a horse\",\n",
    "    \"a photo of a sheep\",\n",
    "    \"a photo of a spider\",\n",
    "    \"a photo of a squirrel\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6829a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_predict(image):\n",
    "    inputs = clip_processor(text=class_names, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        probs = logits_per_image.softmax(dim=1)\n",
    "        return class_names[probs.argmax().item()], probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41dde676",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Sample 100 test examples\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m sample_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m tqdm(sample_dataset):\n\u001b[1;32m      9\u001b[0m     image \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "correct_trained = 0\n",
    "correct_clip = 0\n",
    "total = 0\n",
    "\n",
    "# Sample 100 test examples\n",
    "sample_dataset = dataset['test'].select(range(100))\n",
    "\n",
    "for example in tqdm(sample_dataset):\n",
    "    image = example['image']\n",
    "    true_label = example['label']\n",
    "\n",
    "    # Your model prediction\n",
    "    model_inputs = processor(image.convert(\"RGB\"), return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(**model_inputs).logits.argmax(dim=1).item()\n",
    "\n",
    "    # CLIP prediction\n",
    "    pred_clip_label, _ = clip_predict(image)\n",
    "    clip_label_index = class_names.index(pred_clip_label)\n",
    "\n",
    "    if output == true_label:\n",
    "        correct_trained += 1\n",
    "    if clip_label_index == true_label:\n",
    "        correct_clip += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Trained model accuracy: {correct_trained / total:.2%}\")\n",
    "print(f\"CLIP zero-shot accuracy: {correct_clip / total:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
